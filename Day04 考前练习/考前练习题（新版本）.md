[TOC]

# 1. 准备工作

## 报名注意事项

如何报名考试？

https://training.linuxfoundation.cn/certificate/details/1

注意事项：

 1. 注册账号，填写公司发票或者个人发票

 2. 付款后需要实名认证

 3. 考试报名，推荐名称写英文，比如：Yu Liu ，考试需要检查护照。（写中文名字，需要准备：身份证，信用卡和储蓄卡，两个卡需要有签名）

 4. 考前检查工作：

    ​				Chrome浏览器是设置为允许所有Cookie

    ​				安装PSI Google Chrome Extension（科学上网才能安装）

    ​				检查网络（下载电影，速度2M）

	5. 预约考试时间：提前3天预约，如果预约考试成功但是没有准备准时参加考试，取消考试资格与补考资格。（下次考试在2,498元）

	6. 考试当天提前30分钟自助登陆考试页面

    

## 考试注意事项及考试环境介绍

https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad



# 2. 考试环境介绍

考试环境一共7个cluster，为了尽量减少集群的切换已经对考题进行了分组，同一cluster内的考题将会连续显示。

k8s			1 master,2 worker

hk8s		  1 master,2 worker

bk8s		  1 master,1 worker

wk8s		 1 master,2 worker

ek8s		  1 master,2 worker

mk8s		 1 master,1 worker

ok8s		  1 master,2 worker



每个考题的开头都会使用命令以确保使用正确的cluster，例如：

```shell
kubectl config use-context k8s
```



可以通过 ssh 连接到组成每个cluster的node

```shell
ssh k8s-node
```



使用一下命令可以在任何的node上获取更高的权限

```shell
sudo -i
```





# 3. 练习题

## Task 1

Task weight： 4%

Set configuration context:

```shell
kubectl config use-context k8s
```

Context:

You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.

Task:

Create a new ClusterRole named deployment-clusterrole,which only allows to create the following resource types:

​		Deployment

​		statefulset

​		Daemonset

Create a new ServiceAccount named cicd-token in the existing namespace app-team1.

bind the new ClusterRole deployment-clusterrole to the new Service Account cicd-token,limited to the namespace app-team1.



答案： 

```shell
########## 在kubernetes.io官网搜索rbac
# 切换kubernetes集群
kubectl config use-context k8s

# 创建ServiceAccount
kubectl -n app-team1 create serviceaccount cicd-token

# 创建ClusterRole,并binding给ServiceAccount
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: deployment-clusterrole
rules:
- apiGroups: ["apps"]
  resources: ["deployments","statefulsets","daemonsets"]
  verbs: ["create"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: deployment-clusterrole
subjects:
- kind: ServiceAccount
  name: cicd-token
  namespace: app-team1
roleRef:
  kind: ClusterRole
  name: deployment-clusterrole
  apiGroup: rbac.authorization.k8s.io
  
 # 检查
  kubectl describe clusterrole deployment-clusterrole
  kubectl describe clusterrolebindings.rbac.authorization.k8s.io deployment-clusterrole
```

## Task 2

Task weight: 4%

Set configuration context:

```
kubectl config use-context ek8s
```

Task:

Set the node named ek8s-node-1 as unavailable and reschedule all the pods running on it.



答案：

```shell
# 切换kubernetes集群
kubectl config use-context ek8s

# 设置节点为维护模式
kubectl cordon ek8s-node-1

# 执行下面的命令驱逐应用，执行之后提示什么参数补全参数重新执行。如：kubectl drain ek8s-node-1 --delete-local-data --ignore-daemonsets --force
kubectl drain ek8s-node-1
```

## Task 3

Task weight: 7%

Set configuration context:

```
kubectl config use-context mk8s
```

Task:

Given an existing Kubernetes cluster running version 1.18.8,upgrade all of the Kubernetes control plane and node components on the master node only to version 1.19.0.

You are also expected to upgrade kubelet and kubectl on the master node.

Be sure to drain the master node before upgrading it and uncordon it after the upgrade.

do not upgrade the worker nodes,etcd,the container manager,the CNI plugin,the DNS service or any other addons.



答案：

```shell
########## 在kubernetes.io官网搜索kubeadm upgrade
# 切换kubernetes集群
kubectl config use-context mk8s

# 升级master节点，设置成维护模式并进行应用驱逐
kubectl cordon masternode
kubectl drain masternode

# 查看节点的名称，然后ssh到master节点
kubectl get nodes
ssh master01

######### 配置安装源地址,此段内容考试环境默认已经配置
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update

######### 安装最新kubeadm、kubelet、kubectl,
apt-cache madison kubeadm | less
apt-mark unhold kubelet kubeadm kubectl
sudo apt-get install -y  kubelet=1.19.0-00 kubeadm=1.19.0-00 kubectl=1.19.0-00
apt-mark hold kubelet kubeadm kubectl

######### 升级master节点，不升级etcd
kubeadm upgrade plan  v1.19.0 # 命令查看可升级的版本信息
kubeadm upgrade apply v1.19.0  --etcd-upgrade=false

# 备选项操作：kubectl -n kube-system rollout undo deployment coredns

# 取消master节点的维护模式
kubectl uncordon masternode
```



## Task 4

Task weight: 7%

No configuration context change required for this item.

Task:

First,create a snapshot of the existing etcd instance running at https://127.0.0.1:2379,  SAVING THE SNAPSHOT TO /var/lib/backup/etcd-snapshot.db.

Creating a snapshot of the given instance is expected to complete in seconds.

if the operation seems to hang,something's likely wrong with your command.Use CTRL + C to cancel the operation and try again.

Next, restore an existing,previous snapshot located at /data/backup/etcd-snapshot-previous.db

The following TLS certificates/key are supplied for connecting to the server with etcdctl:

​	CA certificate:

​			/opt/xxxxx/ca.crt

Client certificate:

​			/opt/xxxxx/etcd-client.crt

Client key:

​			/opt/xxxxx/etcd-client.key



答案：

```shell
########## 在kubernetes.io官网搜索etcd
# 准备工作
  docker ps -a | grep etcd
docker cp 52bcd3173184:/usr/local/bin/etcdctl /usr/local/bin/etcdctl

# 备份
ETCDCTL_API=3 etcdctl snapshot save /data/backup/etcd-snapshot.db --endpoints=https://127.0.0.1:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key
# 恢复  
systemctl stop etcd 
systemctl cat etcd # 确认下数据目录（--data-dir值） 
mv /var/lib/etcd /var/lib/etcd.bak 
ETCDCTL_API=3 etcdctl snapshot restore /data/backup/etcd-snapshot-previous.db --data-dir=/var/lib/etcd 
chown -R etcd:etcd /var/lib/etcd 
systemctl start etcd
```



## Task 5

Task weight: 7%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Create a new NetworkPolicy named allow-port-from-namespace that allows Pods in the existing namespace my-app to connect to port 9200 of other pods in the same namespace.

Ensure that the new NetworkPolicy:

​	does not allow access to pods not listening on port 9200

​	does not allow access from pods not in namespace my-app



答案：

```shell
########## 在kubernetes.io官网搜索networkpolicy
# 切换kubernetes
kubectl config use-context k8s


# 开通Port 9200端口访问与my-app命名空间的访问权限
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: my-app
spec:
  ## 这里pod selector不填，则默认选择它所在命名空间下的所有pod
  podSelector:
    matchLabels: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          ## 此处填写my-app命名空间的标签，如果没有此标签执行kubectl lable namespace my-app ns=my-app打标签
          ns: my-app
    ports:
    - protocol: TCP
      port: 9200
```



## Task 6

Task weight: 7%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.

Create a new service named front-end-svc exposing the container port http.

Configure the new service to also expose the individual pods via a NodePort on the nodes on which they are scheduled.



答案：

```shell
########## 在kubernetes.io官网分别搜索static和service
# 切换kubernetes集群
kubectl config use-context k8s

# 添加http 80端口（在image字段下添加）
kubectl edit deployment front-end  
spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        resources: {}

# 创建Service
apiVersion: v1
kind: Service
metadata:
  name: front-end-svc
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    protocol: tcp
    name: http
  selector:
    app: nginx
  type: NodePort
```



## Task 7

Task wight: 7%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Create a new nginx Ingress resource as follows:

​	Name: ping

​	Namespace: ing-internal

​	Exposing service hello on path /hello using service port 80

The availability of service hello can be checked using the following command,which should return hello: curl -kl <INTERNAL_IP>/hello




答案：

```shell
########## 在kubernetes.io官网搜索ingress
# 切换kubernetes集群
kubectl config use-context k8s

# 获取ingressclassname
kubectl get ingressclasses.networking.k8s.io

# 创建ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ping
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx						# 注意：此处执行kubectl get ingressclass查看名字，填写此处，如：nginx
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              number: 80
```



## Task 8

Task weight: 4%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

scale the deployment testpod to 4 pods.



答案：

```shell
# 切换kubernetes集群
kubectl config use-context k8s

# 调整副本数
kubectl scale deployment --replicas=4 testpod
```



## Task 9

Task weight 4%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Schedule a pod as follows:

​	Name: nginx

​	image: nginx

​	Node selector: disk=ssd



答案：

```shell
########## 在kubernetes.io官网搜索nodeselector
# 切换kubernetes集群
kubectl config use-context k8s

# 创建pod
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disk: ssd
```



## Task 10

Task weight 4%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Check to see how many nodes are ready(not including nodes tainted NoSchedule) and write the number to /opt/num.txt.



答案：

```shell
# 切换kubernetes集群
kubectl config use-context k8s

# 查看节点ready数量
kubectl get node

# 查找污点且影响为NoSchedule
kubectl describe nodes xxx | less
 
# 查找如ready的数量减去查找污点且影响为NoSchedule的数量，将数字写入/opt/num.txt文件中
echo 2 > /opt/num.txt
```



## Task 11

Task weight 4%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Create a pod named multi-container with a single app container for each of the following images running in side (there may be between 1 and 4 images specified):

nginx + redis + memcached + consul



答案：

```shell
########## 在kubernetes.io官网搜索static
# 切换kubernetes集群
kubectl config use-context k8s

# 创建pod
apiVersion: v1
kind: Pod
metadata:
  name: multi-container
spec:
  containers:
  - name: nginx
    image: nginx
  - name: redis
    image: redis
  - name: memcached
    image: memcached
  - name: consul
    image: consul
```



## Task 12

Task weight: 4%

Set configuration context:

```
kubectl config use-context hk8s
```

Task:

Create a persistent volume with name app-config,of capacity 4Gi and access mode ReadWriteMany.The type of volume is hostPath and its location is /srv/app-config.



答案：

```shell
########## 在kubernetes.io官网搜索pv
# 切换kubernetes集群
kubectl config use-context hk8s

# 创建pv
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-config
spec:
  storageClassName: manual
  capacity:
    storage: 4Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /srv/app-config
```



## Task 13

Task weight: 7%

Set configuration context:

```
kubectl config use-context ok8s
```

Task:

Create a new PersistenVolumeClaim:

​	Name: pv-volume

​	Class csi-hostpath-sc

​	Capacity: 10Mi

Create a new pod which mounts the PersistenVolumeClaim as a volume:

​	Name: web-server

​	image: nginx

​	Mount path: /usr/share/nginx/html

Configure the new pod to have ReadWriteOnce access on the volume.

Finally,using kubectl edit or kubectl patch expand the PersistentVolumeClaim to a capacity of 70Mi and record that change.



答案：

```shell
########## 在kubernetes.io官网分别搜索pv和storageclass
# 切换kubernetes
kubectl config use-context ok8s

# 创建pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  storageClassName: csi-hostpath-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
      
# 创建pod
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: pv-volume
  containers:
    - name: task-pv-container
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: task-pv-storage

# 修改容量前，需要先更新storageclass
kubectl edit storageclass csi-hostpath-sc
# 注意检查一下字段是否添加，如果没有需要添加下列字段
allowVolumeExpansion: true

# 修改pvc容量
kubectl edit pvc pv-volume --record

```



## Task 14

Task weight: 5%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

Monitor the logs of pod bar and:

​	Exract log lines corresponding to error file-not-found

​	Write them to /opt/test/bar



答案：

```shell
# 切换kubernetes集群
kubectl config use-context k8s

# 查看logs
kubectl logs bar | grep file-not-found > /opt/test/bar
```



## Task 15

Task weight 7%

Set configuration context:

```
kubectl config use-context k8s
```

context:

Without changing its existing containers,an existing pod needs to be integrated into kubernetes's built-in logging architecture (e.g.kubectl logs).Adding a streaming sidecar container is a good and common way to accomplish this requirement.

Task:

Add a busybox sidecar container to the existing pod legacy-app.The new sidecat container has to run the following command:

```
/bin/sh -c tail -n+1 /var/log/legacy-app.log
```

Use a volume mount named logs to make the file /var/log/legacy-app.log available to the sidecar container.

Don't modify the existing container.

Dont modify the path of the log file,both containers must access it at /var/log legacy-app.log.



答案：

```shell
########## 在kubernetes.io官网搜索logging
# 切换kubernetes集群
kubectl config use-context k8s

# 模拟考试环境创建已经存在的pod，名字为legacy-app
apiVersion: v1
kind: Pod
metadata:
  name: legacy-app
spec:
  containers:
  - name: logs-busybox
    image: busybox:1.28
    command: ['sh', '-c', "tail -n+1 /var/log/legacy-app.log"]
    volumeMounts:
    - name:  logs
      mountPath: /var/log/
  - name: container01
    image: busybox:1.28
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/legacy-app.log;
        i=$((i+1));
        sleep 1;
      done      
    volumeMounts:
    - name: logs
      mountPath: /var/log
  volumes:
  - name: logs
    emptyDir: {}
    
###################### 以下为正式答题内容 ###############################
# 将legacy-app以yaml格式输出并保存到本地
kubectl get pod legacy-app -o yaml > test.yaml

# 编辑本地的test.yaml文件添加container
  - name: logs-busybox
    image: busybox:1.28
    command: ['sh', '-c', "tail -n+1 /var/log/legacy-app.log"]
    volumeMounts:
    - name:  logs
      mountPath: /var/log/
      
#  删除legacy-app后，重新创建legacy-app
kubectl delete pod legacy-app;kubectl apply -f test.yaml
```



## Task 16

Task weight: 5%

Set configuration context:

```
kubectl config use-context k8s
```

Task:

From the pod label name=overloaded-cpu,find pods running high cpu workloads and write the name of the pod consuming most cpu to the file

/opt/test1.txt (which already exists).



答案：

```shell
# 切换kubernetes集群
kubectl config use-context k8s

# 查看pod名称
kubectl top pod -l name=overloaded-cpu --all-namespaces --sort-by=cpu

# 将cpu占用最多的pod的name写入/opt/test1.txt文件
echo test-pod > /opt/test1.txt
```



## Task 17

Task weight: 13%

Set configuration context:

```
kubectl config use-context wk8s
```

Task

A kubernetes woker node,named wk8s-node-0 is in state Not Ready.Investigate why this is the case,and perform any appropriate steps to bring the node to a Ready state,ensuring that any changes are made permanent.

You can ssh to the failed node useing: 

```
ssh wk8s-node-0
```

You can assume elevated privileges on the node with the following command:

```
sudo -i
```



答案：

```shell
# ssh登陆节点
ssh wk8s-node-0

# 提成权限
sudo -i

# 启动服务，并设置为开机自启动
systemctl restart kubelet ; systemctl enable kubelet
```

